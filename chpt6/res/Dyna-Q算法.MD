其实这张章节就是为了引入一个基于模型的强化学习
1、无模型的强化学习和基于模型的强化学习  
在强化学习中，模型通常指与智能体进行交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境魔心，强化学习算法分为两种：基于模型的强化学习（model-based reinforcement learning）
和无模型的强化学习model-free reinforcement learning。无模型的强化学习根据智能体与环境交互采样得到的数据直接进行策略提升或者价值估计。时序差分算法就是典型的无模型强化学习。
大多数强化学习也是无模型的强化学习。在基于模型的强化学习中，模型是可以事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后利用这个模型帮助我们进行策略提升或者价值估计。
动态规划两种算法：策略迭代和价值迭代就属于基于模型的强化学习，这两种算法环境中模型是事先已知的。这章节的Dyna-Q算法是风场基础的基于模型的强化学习算法，不过他的环境模型是通过采样数据得到的。

2、强化学习的评价指标  
（1）算法收敛后的策略在初始状态下的期望回报 （2）样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，
对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是环境模型可能不准确，不能完全替代真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报
可能不如无模型的强化学习算法

3、Dyan-Q 
Dyan-Q基于Q-planning来基于模型生成一些模拟数据，然后用模拟属于和真实数据一起改进策略。Q-planning每次选取一个曾经访问过的状态s，采取一个曾经在该状态下执行过的动作a，
通过模型得到转移后的状态s'以及奖励r，并根据这个模拟数据(s,a,r,s')，用Q-learning的更新方式来更新动作价值函数

4、结论
    随着Q-planning步数增多，Dyna-Q算法的收敛速度变快，但是这取决于环境是否是确定性的，以及环境模型的精度。在悬崖漫步中，状态的转移是完全确定的，构建的环境模型精度最高，所以可以通过增加Q-planning步数来直接降低算法的样本复杂度。