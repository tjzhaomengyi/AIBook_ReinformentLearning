1、简介
之前介绍的都是单智能体强化学习算法，其基本假设是动态环境是稳态的，即状态转移概率和奖励函数不变，并依此来设计相应的算法。而
如果环境中还有其他智能体进行交互和学习，那么任务则上升为多智能体强化学习multi-agent reinforcement learning,MARL.
多智能体的情形相比于单智能体更加复杂，因为每个智能体在和环境交互的同时也在和其他智能体进行直接或者间接额交互。因此，多智能体强化学习要比
单智能体强化学习要更困难，其难点在于以下几点：（1）由于多个智能体在环境中进行实时动态交互，并且每个智能体在不断学习并更新自身策略，因此
在每个智能体的视角下，环境是非稳态的，即对一个智能体而言，即使在相同的状态下采取相同的动作，得到的状态转移和奖励信号的分布也有可能在不断改变。
（2）多个智能体的训练可能是多目标的，不同智能体需要最大化自己的利益；（3）训练评估的复杂度会增加，可能需要大规模分布式训练来提升效率。

2、问题建模
将一个多智能体环境用一个元组（N，S，A，R，P）表示，其中N是智能体的数目，S=S1*S2*...*Sn是所有智能体的状态集合，A=A1*A2*...AN是所有智能体的动作集合，
R=r1 * r2 *....*rn是所有智能体奖励函数的集合，P是环境的状态转移概率。一般多智能体强化学习的目标是为每个智能体学习一个策略来最大化其自身的累积奖励。

3、多智能体强化学习的基本求解范式  
面对上述问题形式，最直接的思想是基于已经熟悉的单智能体算法来进行学习，主要分为两种思路：
（1）完全中心化方法：将多个智能体进行决策当作一个超级智能体进行决策，即把所有智能体的状态结合在一起当作一个全局的超级状态，把所有智能体的动作连起来作为一个联合动作。这样做的
好处是，由于已经知道了所有智能体的状态和动作，因此对这个超级智能体来说，环境依旧是稳态的，一些单智能体的算法的收敛性依旧可以得到保证。然而，这种做法不能很好地扩展到智能体
数量很多或者环境很大的情况，因为这个时候所有的信息简单暴力地拼在一起会导致维度爆炸，训练复杂度巨幅提升的问题往往不可解决。
（2完全去中心化方法：与完全中心化方法相反的范式是假设每个智能体都在自身的环境中独立地进行学习，不考虑其他智能体的改变。完全去中心化直接对每个智能体用一个智能体强化学习算法来学习。
这样做的缺点是环境是非稳态的，训练的收敛性不能得到保证，但是这种方法的好处在于随着智能体数量的增加会有比较好的扩展性，不会遇到维度灾难而导致训练不能进行下去。

4、IPPO算法
IPPO是一个完全去中心化的算法，此类算法被称为独立学习。由于对于每个智能体使用单智能体算法PPO进行训练，所以这个算法叫做独立PPO（Independent PPO，IPPO）。具体而言，这里使用的
PPO算法为PPO-截断，伪代码：  
对于N个智能体，为每个智能体初始化各自的策略以及价值函数  
for 训练轮数 k = 0，1，2，.... do   
&emsp; 所有智能体在环境中交互分别获得各自的一条轨迹数据  
&emsp; 对每个智能体，基于当前的价值函数用GAE计算优势函数的估计  
&emsp; 对每个智能体， 通过最大化其PPO-截断的目标来更新其策略  
&emsp; 对每个智能体， 通过均方误差损失函数优化其价值函数  
end for

5、结论
当智能体数量较少的时候，IPPO这种完全去中心化学习在一定程度上能够取得好的效果，但是最终达到得胜率也比较有限，
这可能是因为多个智能体之间无法有效地通过合作来共同完成目标。并且当智能体数量增加到5时，这种完全去中心化学习的效果就不是
很好了。这是就需要引入更多的算法来考虑多个智能体之间的交互行为，或者使用中心化训练去中心化执行的范式进行多智能体训练。