DQN算法在2013年提出，有很多不足，后续涌现很多改进算法，比较好的是Double DQN和 Dueling DQN。普通DQN算法是对Q值的过高估计。  
前提公式 在传统DQN中r + γQw-(s',a') ，其中Qw-可以写成Qw-(s',argmaxa'Qw-(s',a'))
1、DoubleDQN的改进思路
DoubleDQN提出利用例两套独立训练的神经网络来估算maxa'Q(s'a'),具体做法是将原来的maxa'Qw-(s',a')改为Qw-(s',argmaxa'Qw-(s',a')),
即利用一套神经网络Qw的输出来选取价值最大的动作，但是在使用该动作的价值时，用另一套神经网络Qw-来计算该动作的价值。这样，即使其中一套神经网络中的某个动作存在比较严重的过高估计问题，
由于另一个神经网络存在，这个动作最终使用的Q值夜不会存在很大的过高估计问题。  
在传统的DQN算法中，本来就存在两套函数Q的神经网络---训练网络和目标网络，只不过maxQw-(s',a')的计算只用到了其中的目标网络，那么我们恰好可以直接将训练网络作为
DoubleDQN算法中的第一套神经网络来选取动作，将目标网络作为第二套神经网络来计算Q值，这便是DoubleDQN的主要思想。由于DQN算法将训练网络的参数记为w，并将
目标网络的参数纪委w-，这与本节中DoubleDQN的两套神经网络的参数是同意的，因此我们可以直接写出入下的Double DQN优化目标 r + γQw-（s'，argmaxQ(s', a')）  
所以传统DQN的公式r+Qw-(s',argmaxa'Qw-(s',a'))和Double DQN的公式 r + γQw-（s'，argmaxQ(s', a'))的区别,DQN动作选取依靠目标网络，Double DQN的动作选取依赖训练网络。  
2、Dueling DQN  【2015】
在强化学习中，我们将动作价值函数Q减去状态价值函数V的结果定义为优势函数A，即A(s,a)=Q(s,a)-V(s).在同一状态下，所有动作的优势值之和为0，因为，所有动作的动作价值期望
就是这个状态的状态价值。因此，在Dueling DQN中，Q网络被建模为Qη,α,β(s,a)=Vη,α(s) + Aη,β(s,a),Aη,β(s,a)表示采取不同动作的差异性；η是状态价值函数和优势函数共享的网络参数u，
一般在神经网络中，用来提取特征的前几层；而α和β分别为状态价值函数和优势函数的参数。在这样的模型下，我们不再让神经网络直接输出Q值，而是训练神经网络的最后几层的两个分支，
分别输出状态价值函数和优势函数，再求和得到Q值。Dueling DQN的网络结构图见数，最后两层的连接非常稀疏。
    【利用优势函数建模的优势】将状态价值函数和优势函数分别建模的好处在于：某些环境下智能体只会关注状态价值，而不关注不同动作导致的差异，此时将二者分开能够使得智能体更好地处理与动作关联较小的状态。
比如书中的例子中，在对面无车的自动驾驶中，智能体更关注状态价值（前面讲的关注状态价值的强化学习模型），但是当对面有来车或者要超车的时候，只能提要开始关注不同动作优势值的差异（Dueling的优势）
3、Dueling DQN对最优动作函数的处理Qη,α,β(s,a)=Vη,α(s) + Aη,β(s,a) -maxAη,β(s,a')强制这个最优动作的优势函数的实际输出是0.此时V(s)=maxQ(s,a)可以保证V值建模的唯一性。此外还可以利用平均操作代替最大操作
V(s)=1/|A|∑Q(s,a).虽然这个转换不满足贝尔曼最优方程，但是更稳定【有点背离】。

3、Dueling DQN为什么比DQN好  
   部分原因在于Dueling DQN能更高效地学习状态价值函数。每一次更新时，函数V都会被更新，这也会影响到其他动作的Q值。而传统的DQN只会更新某个动作的Q值，其他动作的Q值就不会更新。因此，Dueling DQN能够更加
频繁、准确地学习状态价值函数。

4、结论相比于传统的DQN，Dueling DQN在多个动作选择下的学习更加稳定，得到的回报最大值也更大。由Dueling DQN的原理可知，随着动作空间的增大，Dueling DQN比DQN的优势更为明显。
我们可以增加离散动作数（这里都是11），可以变成15、25.  

5、总结  
Dueling DQN能够很好学习不同动作的差异性，在动作空间较大的环境下非常有效。从Double DQN 和 Dueling DQN的方法原理中，我们感受到强化学习的研究关注如何将强化学习和深度学习有效结合：
一是在深度学习模块的基础上，强化学习算法如何更加有效地工作，并避免深度学习模型行为带来的一些问题，例如使用Double DQN解决Q值过高估计的问题。
二是在强化学习的场景下，深度学习模型如何有效地学习到有用的模式，例如设计Dueling DQN网络架构来高效地学习状态价值函数以及动作优势函数。
    