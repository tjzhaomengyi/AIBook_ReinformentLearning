1、魔模仿学习简介  
假设存在一个专家智能体，其策略可以看成最优策略，我们就可以通过直接模仿这个专家在环境中交互的状态动作数据来训练一个策略，并且
不需要用到环境提供的奖励信号。模仿学习研究的就是这一类问题，在模仿学习的框架下，专家能够提供一系列状态动作对{(st,at)},表示专家
在st环境下做出了at的动作，而模仿者的任务则是利用这些专家数据进行训练，无需奖励信号就可以达到一个接近专家的策略。
学术界的模仿学习的方法基本上分为三类：行为克隆、逆强化学习inverse Rl、生成对抗模仿学习generative adversarial imitation learning，GAIL。
主要介绍行为克隆方法和生成对抗模仿学习方法。  
2、行为克隆
行为克隆学习就是直接使用监督学习方法，将专家数据中(st,at)的st看做样本输入，at卡诺标签，学习的目标为θ*=argminE(s,a)~B[L(πθ(s),a)]
其中，B是专家的数据集，L是对应监督学习框架下的损失函数。若动作是离散的，该损失函数可以通过最大似然函数得到。若动作是连续的，该损失函数可以使均方误差函数。
在训练数据量比较大的时候，BC能够快速学习到一个不错的策略。例如，围棋人工智能AlphaGo，在16万盘棋局中的3000万次落子数据中学习人类如何下棋。BC的实现简单，因此在很多
实际的场景下它都可以作为策略预训练的方法，BC能够使得策略无须在较差时候仍然低效地通过和环境交互来探索较好的动作，而是通过模仿
专家智能体的行为数据来快速到达较高的水平，为接下来的强化学习创造一个高起点。  
BC的局限性：数据量较小的时候较为明显。具体来说，由于通过BC学习得到的策略只是拿小部分专家数据进行训练，因此BC只能在专家数据的状态分布下预测
得比较准确。然而，强化学习面对的是一个序贯决策问题，通过BC学习得到的策略在和环境交互过程中不可能完全学成最优，只要存在一点偏差，就有可能导致下一个遇到的状态是在专家
数据中没有出现过的。此时由于没有在此状态或者比较相近的状态下训练过，策略可能就会随机选择一个动作，这会导致下一个状态进一步偏离专家策略
遇到的数据分布。最终，该策略在真实环境下不能得到较好的结果，这被称为行为克隆的复合误差。

3、生成对抗模仿学习GAIL
2016年由斯坦福大学提出的基于生成对抗网络的模仿学习，它诠释了生成对抗网络的本质其实就是模仿学习。GAIL实质上是模仿了专辑策略的占用度量
ρE(s,a),即尽量使得策略在环境中的所有状态对（s,a）的占用度量ρπ(s,a)和专家策略的占用度量一致。为了达成这个目标，策略需要和环境进行交互。
GAIL算法中有一个判别器和一个策略，策略π箱单与生成对抗网络中的生成器，给定一个状态，策略会输出这个状态下应该采取的动作，而判别器D将
状态动作对（s,a）作为输入，输出一个0到1的实数，表示判别器任务该状态动作对(s,a)是来自智能体策略而非专家的概率。判别器D的目标是
尽量将专家数据的输出靠近0，将模仿者策略的输出靠近1，这样就可以将两组数据分辨出来。于是，判别器的损失函数为
L(φ)=-Eρπ[logDφ(s,a)]-Eρe[log1-Dφ(s,a)]
其中，φ是判别器D的参数。有了判别器D之后，模仿者策略的目标就是其交互产生的轨迹能被判别器误认为专家轨迹。于是，我们可以用判别器D的输出
作为建立函数来训练模仿者的策略。具体来说，若模仿者策略在环境中采样到状态s，并且采取动作a，此时该状态动作对(s,a)会输入到判别器D中，
输出D(s,a)的值，然后将奖励设置为r(s,a)=-logD(s,a)。于是，我们可以用任意强化学习算法，使用这些数据继续训练模仿者策略。
最后在对抗过程中不断进行后，模仿者策略生成的数据分布将接近真实的专家数据分布，达到模仿学习的目标。
第3章介绍过一个策略和给定MDP交互的占用度量呈一一对应的关系，因此，模仿学习的本质就是通过更新策略使其占用度量尽量靠近专家的占用度量，而这正是GAIL的训练目标。
由于一旦策略改变，其占用度量就会改变，因此为了训练好最新的判别器，策略需要不断和环境进行交互，采样最新的状态动作对样本。

4、结论
我们发现BC无法学习到最优策略，主要是因为在数据量比较少的情况下学习容易发生过拟合。
在数据样本有限的情况下，BC不能学习到最优策略，但是GAIL在相同的专家数据下可以取得非常好的结果。这一方面归因于GAIL的训练目标，拉进策略和专家的
占用度量，十分贴合模仿学习任务的目标，避免了BC中的复合误差问题；另一方面得益于GAIL的训练中，策略可以和环境交互出更多的数据，
以此训练判别器，进而生成对基于策略量身定做的知道奖励信号。