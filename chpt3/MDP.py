# -*- coding: utf-8 -*-
__author__ = 'Mike'
#根据书中图3-4马尔可夫决策过程设计一个简单例子
import numpy as np

S = ["s1", "s2", "s3", "s4", "s5"]#状态集合
A = ["保持s1", "前往s1", "前往s2", "前往s3", "前往s4", "前往s5", "概率前往"]#工作集合

#状态转移函数
P = {
    "s1-保持s1-s1":1.0, "s1-前往s2-s2":1.0,
    "s2-前往s1-s1":1.0, "s2-前往s3-s3":1.0,
    "s3-前往s4-s4":1.0, "s3-前往s5-s5":1.0,
    "s4-前往s5-s5":1.0, "s4-概率前往s2-s2":0.2,
    "s4-概率前往s3":0.4, "s4-概率前往-s4":0.4
}

#奖励函数
R = {
    "s1-保持s1":-1, "s1-前往s2":0,
    "s2-前往s1":-1, "s2-前往s3":-2,
    "s3-前往s4":-2, "s3-前往s5":0,
    "s4-前往s5":0.5, "s4-概率前往":0.5
}

#策略1，随机策略
Pi_1 = {
    "s1-保持s1":0.5, "s1-前往s2":0.5,
    "s2-前往s1":0.5, "s2-前往s3":0.5,
    "s3-前往s4":0.5, "s3-前往s5":0.5,
    "s4-前往s5":0.5, "s4-概率前往":0.5
}

#策略2
Pi_2 = {
    "s1-保持s1":0.6, "s1-前往s2":0.4,
    "s2-前往s1":0.3, "s2-前往s3":0.7,
    "s3-前往s4":0.5, "s3-前往s5":0.5,
    "s4-前往s5":0.1, "s4-概率前往":0.9
}

#把输入的两个两个字符通过"-"连接，便于使用上述定义的P、R变量
def join(str1, str2):
    return st1 + '-' + str2


#2、计算value马尔可夫奖励过程中所有状态的价值
def compute_value(P, rewards, gamma, states_num):
    "利用贝尔曼方程的举证形式计算解析解，state_num是MRP的状态数"
    rewards = np.array(rewards).reshape((-1, 1)) #将rewards写成向量形式
    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)
    return value

#x下面我们要计算下该MDP下一个策略π的状态价值函数。我们现有的工具是MRP的解析解方法。想法：给定一个MDP和一个π，是否可以将其转化为一个MRP。
#答案是肯定的。我们将策略的动作选择进行边缘化marginalizaion，就可以得到没有动作的MRP。对于某一个状态，我们根据策略将所有动作的概率进行加权,
#得到的奖励和就可以被认为是一个MRP在该状态下的奖励。
# 同理，我们计算采取动作的概率与使s转移到s‘的该来的成绩，再将这些乘积相加，其和就是一个MRP的状态从s转移到s'的概率。
# 于是我们从MDP得到了一个MRP。于是我们可以利用MRP中计算价值函数的解析解来解这MDP中策略的状态价值函数。
gamma = 0.5


#转化后的MRP的状态转移矩阵,选择pi_1策略
P_from_mdp_to_mrp = [
    [0.5, 0.5, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.5, 0.5],
    [0.0, 0.1, 0.2, 0.2, 0.5],
    [0.0, 0.0, 0.0, 0.0, 1.0]
]
P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)
R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]

V = compute_value(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)
print("MDP中每个状态价值分别为\n", V)
# MDP中每个状态价值分别为
#  [[-1.22555411]
#  [-1.67666232]
#  [ 0.51890482]
#  [ 6.0756193 ]
#  [ 0.        ]]
'''
# 在得到状态价值函数后，可以计算得到动作价值函数Qπ(s,a)，s4概率前往的动作价值:
#   2.152 = 1 + 0.5*[0.2*(-1.68) + 0.4 * 0.52 + 0.4 * 6.08]
#   1表示奖励函数R中“s4-概率前往”=1
# 0.2|0.4|0.4表示在状态s选择动作a后转移到状态s'的概率：（1）0.2表示s4-概率前往-s2 （2）0.4表示s4-概率前往-s3 （3）第二个0.4表示s4-概率前往-s4
# -1.68 | 0.52 | 6.09表示到达状态s'的价值 （1）-1.68表示s2的价值，（2）0.52表示s3的价值 （3）6.08表示s4的价值
'''