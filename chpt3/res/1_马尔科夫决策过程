1、马尔科夫决策过程就是强化学习环境中的决策过程。马尔科夫决策过程包含状态信息以及状态之间的转移机制。如果要用强化学习解决问题，就是要把问题明确为马尔科夫决策过程的各个组成要素。
2、马尔可夫性质
3、马尔可夫过程【状态转移矩阵】：指具有马尔可夫性质的随机过程，也被称为马尔可夫链。给定一个马尔可夫过程，我们就可以从某个状态除法，根据它的状态转移矩阵生成一个状态序列episode，这个步骤也被叫做采样。
4、马尔可夫奖励过程：在马尔可夫过程的基础上加入奖励函数r和折扣因子γ，就可以得到马尔科夫奖励过程Markov reward process,MRP.一个马尔可夫过程由<S,P,r,γ>，各个组成元素，
    S是有限状态的集合。p是状态转移矩阵。r是奖励函数，某个状态s的奖励r(s)指转移到该状态时可以获得奖励的期望。y是折扣因子。
    （1）回报Return，在一个马尔可夫奖励过程中，从第t时刻状态St开始，直到终止状态时，所有奖励的衰减之和称为回报Return
    （2）价值函数Value,在马尔可夫奖励过程中，一个状态的期望回报被称为这个状态的价值。所有状态的价值组成了价值函数value function，价值函数的输入为某个状态，输出为这个状态的价值。【贝尔曼方程】
5、马尔可夫决策过程：（1）马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的刺激来共同改变这个随机过程，就有了马尔可夫决策过程Markov decision process MDP。
我们将这个来自外界的刺激称为智能体agent的动作，在马尔可夫奖励过程MRP的基础上加入动作，就得到了马尔可夫决策过程MDP。马尔可夫决策过程由元组<S,A,P,r,γ>构成。S是状态的集合；A是动作的集合；
γ是折扣因子，r(s,a)是奖励函数，此时奖励可以同时取决于状态s和动作a，在奖励函数值只取决于状态s时，则退化为r(s)【马尔克夫奖励过程】;P(s'|s,a)是状态转移函数，表示在状态s执行a动作之后达到状态s'的概率
（2）理解马尔可夫决策过程和马尔可夫奖励过程。
    【状态转移函数】MDP中的状态转移函数和奖励函数都比MRP多了作为机子变量的动作a。在MDP的定义中，我们不再使用类似MRP定义中的状态转移矩阵，而是直接使用状态转移函数。这样做一是因为此时的状态转移
    与动作也有关，变成了一个三维数组，而不再是一个矩阵（二维数组）；二是因为状态转移函数更具有一般意义。例如，如果状态集合不是有限的，就无法用数组表示，但是仍然可以用状态转移函数表示。我们之后会
    遇到连续状态的MDP环境，那时集合状态都不是有限的。现在我们主要关注离散状态的MDP环境，此时状态集合是有限的。
   【MDP过程】不同于马尔克夫奖励过程，在马尔可夫过程中，通常存在一个智能体来执行动作。例如，小船在大海中随着水流自由飘荡过程就是一个马尔可夫奖励过程，如果凭借运气到了一个目的地，就能获得比较大的奖励。
   马尔科夫过程就是一个与详见相关的不断进行的过程，在智能体和环境MDP之间存在一个不断交互的过程。一般而言，它们之间的交互过程：智能体根据当前状态St选择动作At；对于状态St和At，MDP根据奖励函数
   和状态转移函数得到St+1 和 Rt 并反馈给智能体。智能体的目标是最大化得到的累积奖励的期望。
   【MDP的策略】智能体根据当前状态从动作的集合A中选择一个动作的函数，称为策略。
（3）策略policy，通常用π表示。π(a|s) = P(At=a|St=s)是一个函数，表示在输入状态为s的情况下采取动作a的概率。当一个策略是确定性策略时，它在每个状态下只输出一个确定性的动作，即只有该动作的概率
为1，其他动作概率为0；当一个策略是随机性策略stochastic policy时，它在每个状态下输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。在MDP中，由于马尔可夫性质的存在，策略
只需要与当前状态有关，不需要考虑历史状态。在MDP中也有和MRP中类似的价值函数，但是此时的价值函数与策略有关，意味着对两个不同的决策来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，
因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。
（4）在MDP中基于策略π的状态价值函数，表示为从状态s出发遵循策略π能够获得的期望回报：Vπ(s) = Eπ[Gt|St=s]。
（5）在MDP中的动作价值函数，Qπ(s,a)表示在MDP遵循策略π时，对当前状态执行动作a得到的期望回报：Qπ(s,a)=Eπ[Gt|St=s,At=a]
（6）状态价值函数和动作价值函数之间的关系：在使用策略π时，状态s的价值等于在该状态下基于策略π采取所有动作的概率与相应的价值相乘再求和的结果:Vπ(s)=Σ(a∈A)π(a|s)Qπ(s,a)
（7）使用策略π时，在状态s下采取动作a的价值等于即时奖励加上经过衰减的所有可能的下一个状态的转移概率与相应的价值的乘积Qπ(s,a)=r(s,a)+γΣ(s'∈S)P(s'|s,a)Vπ(s')
（8）贝尔曼期望方程，利用贝尔曼期望方程可以得到两个价值函数的贝尔曼期望方程
（9）MDP价值求解向MRP解析解的转化：下面我们要计算下该MDP下一个策略π的状态价值函数。我们现有的工具是MRP的解析解方法。想法：给定一个MDP和一个π，是否可以将其转化为一个MRP。
#答案是肯定的。我们将策略的动作选择进行边缘化marginalizaion，就可以得到没有动作的MRP。对于某一个状态，我们根据策略将所有动作的概率进行加权,
#得到的奖励和就可以被认为是一个MRP在该状态下的奖励。
# 同理，我们计算采取动作的概率与使s转移到s‘的该来的成绩，再将这些乘积相加，其和就是一个MRP的状态从s转移到s'的概率。
# 于是我们从MDP得到了一个MRP。于是我们可以利用MRP中计算价值函数的解析解来解这MDP中策略的状态价值函数。
（10）最终将MDP问题转化为MRP问题即可求得其状态价值函数，然后根据价值函数计算得到策略价值Q，参考代码2_MDP最后结果部分。
（11）抛出问题：这个MRP解析解的方法在状态动作集合比较大的时候不是很实用，问题复杂后求MDP转化的MRP的状态价值就变得困难。
    planA：我们可以用动态规划的方法得到价值函数
    planB：可以选择用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道MDP的状态转移函数和奖励函数，它可以得到一个近似值，并且采样越多越准确

6、蒙特卡洛方法
    （1）蒙特卡洛方法的思路，就是抽样概率
    蒙特卡洛方法也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想得到的目标的数值估计。
最经典的例子就是用正方形来求其直径的圆面积，圆面积/正方形面积=园中点的个数/正方形中点的个数。
    如何用蒙特卡洛方法来规矩一个策略在一个马尔可夫决策过程中的状态价值。一个状态的价值是它的期望回报，那么一个直观的想法就是用策略在MDP上采样很多条序列，计算从这个状态出发的回报在求其期望就可以了。
    Vπ(s) = Eπ[Gt|St=s]≈1/N * Σ(i=1到N)Gt(i)
    （2）蒙特卡洛算法的方法
    PlanA：在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现很多这个状态。蒙特卡洛价值价值估计方法会在该状态每一次出现时计算它的回报。
    PlanB:一条序列只计算一次回报，也就是在这条序列第一次出现该状态时计算后面的累积奖励，而后面再出现该状态时，忽略该状态。

7、占用度量【占用度量的意义：估计每个策略的占用量】
    不同策略的价值函数是不一样的，这是因为对于同一个MDP，不同策略会访问到的状态的概率分布是不同的。假设在书中图3-4中的MDP中现在有一个策略，它的动作执行会使得智能体尽快到达终止状态s5，于是当智能体处于状态
s3时，不会采取前往s4的动作，而是只会以1的概率采取前往s5，智能体也不会获得在s4状态下采取“前往s5”可以得到的很大的奖励10.根据贝尔曼方程，这个状态在s3的概率会比较小，究其原因是智能体无法到达状态s4.
因此我们需要理解不同策略会使得智能体访问到不同概率分布的状态这个事实，会影响到策略的价值函数。
   占用度量中的几个定义：（1）定义MDP的初始状态分布v0(s),（2）用Ptπ(s)表示采取策略π使得智能体在t时刻状态为s的概率，在初始分布的时候有P0π(s)=v0(s)，（3）然后可以定义一个策略的状态访问分布（state visittion distribution）
状态访问概率表示在一个策略下智能体和MDP交互会访问到的状态的分布。（4）智能体和MDP的交互在一个序列中是有限的。（5）可以定义策略的占用度量occupancy measure，表示状态动作(s,a)被访问到的概率。

8、强化学习的最优策略
    强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。在有限状态和动作集合的MDP中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个
策略就是最优策略。最有策略可能有很多个，我们都将其表示为π*(s)。
    最优策略都有相同的状态价值函数，称之为最有状态价值函数V*(s)。同理定义最有动作价值函数Q*(s,a)。为了使Q*(s,a)最大，我们需要再当前的状态动作对（s, a）之后都执行最优策略。于是我们得到了
最优状态价值函数和最优动作价值函数之间的关系Q*(s,a) = r(s,a) + γ∑P(s'|s,a)V*(s')【这个是强化学习的重要结论函数--Q和V的关系】。这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。
另一方面，最优状态价值是选择此时最有动作价值最大的那一个动作时的状态价值：V*(s)=maxQ*(s,a)
9、贝尔曼最优方程
    根据V*(s)和Q*(s,a)的关系，我们可以得到贝尔曼最优方程（略）p33